[Unit]
Description=LlamaForge Distributed Training Worker
After=network.target ollama.service
Wants=ollama.service

[Service]
Type=simple
User=joker
Group=joker
WorkingDirectory=/home/joker/LlamaForge

# Environment
Environment="PYTHONUNBUFFERED=1"
Environment="MASTER_ADDR=10.9.66.154"
Environment="MASTER_PORT=29500"
Environment="NODE_RANK=1"
Environment="WORLD_SIZE=2"

# The service waits for training jobs to be launched
# This is a persistent worker that listens for DDP connections
ExecStart=/usr/bin/python3 -m torch.distributed.run \
  --nproc_per_node=1 \
  --nnodes=${WORLD_SIZE} \
  --node_rank=${NODE_RANK} \
  --master_addr=${MASTER_ADDR} \
  --master_port=${MASTER_PORT} \
  --rdzv_backend=c10d \
  --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
  distributed_train.py \
  --auto-resume

# Restart policy
Restart=on-failure
RestartSec=10
StartLimitInterval=200
StartLimitBurst=5

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=llamaforge-worker

[Install]
WantedBy=multi-user.target
